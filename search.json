[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kiwan Park",
    "section": "",
    "text": "news\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2022\n\n\nKiwan Park\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2022\n\n\nKiwan Park\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2022\n\n\nKiwan Park\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2022\n\n\nKiwan Park\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Assignment/index.html",
    "href": "posts/Assignment/index.html",
    "title": "Assignment",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/Assignment3/index.html",
    "href": "posts/Assignment3/index.html",
    "title": "Assignment3",
    "section": "",
    "text": "[EX1](https://images.squarespace-cdn.com/content/v1/59413d96e6f2e1c6837c7ecd/1555456455354-FFFX3A8XF4LA6672RJJ4/mantel_blue_173646.png?format=1000w)\n[EX2](https://images.squarespace-cdn.com/content/v1/5c12933f365f02733c923e4e/1548697680570-FQY4FY8EK4SEJOTW9CR8/EssayHeader_2.jpg?format=1000w)\nQ2.\n\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 <- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 <- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 <- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 <- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n## Fancy version (per help file)\n\nff <- y ~ x\nmods <- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] <- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] <- as.name(paste0(\"y\", i))\n  ##      ff[[3]] <- as.name(paste0(\"x\", i))\n  mods[[i]] <- lmi <- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(>F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] <- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"green\", pch = 21, bg = \"red\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"black\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\npar(op)\n\n############################\n\nhpi <- read.csv(\"HPI_GDP.csv\")\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n✔ purrr   0.3.4      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nhappy <- hpi %>%filter(year == 2017) %>% ggplot(aes(y = HPI, x = GDP, colour = Continent)) +geom_point(alpha = 0.3) +theme_bw() +geom_smooth(method = \"lm\", se = FALSE) +scale_colour_brewer(palette = \"Set1\") +labs(x = \"GDP per capita\",y = \"Happy Index\",title = \"Economics and Happy\",subtitle = \"Happy Planet Index, 2017\",caption = Sys.Date())\n\nhappy\n\n`geom_smooth()` using formula 'y ~ x'\n\n\nWarning: Removed 8 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 8 rows containing missing values (geom_point).\n\n\n\n\n\nQ3. Critique\n[This is the pros and cons of the establishment of the “Corruption Investigation Office for High-ranking Officials(CIO)” announced in March 2019. Despite the overwhelming public opinion in favor, it was marked to give the impression that the opposition was also very strongly formed. Although most of the public had wished that the establishment of the CIO would strongly investigate corruption among high-ranking government officials, some vested interests and conservative media had tried to undermine such public willingness. Eventually, the CIO was established in January 2021."
  },
  {
    "objectID": "posts/Assignment4/index.html",
    "href": "posts/Assignment4/index.html",
    "title": "Assignment4",
    "section": "",
    "text": "# Data Visualization\n# Assignment 4-1\n\n# Set Column Names\ndf <- data.frame(x = c(\"Texas\", \"Florida\", \"NewYork\",\n     \"California\"), width = c(25, 50, 75, 100), height = c(100,\n     75, 50, 25))\n\ndf$w <- cumsum(df$width)\ndf$wm <- df$w - df$width\ndf$wt <- with(df, wm + (w - wm)/2)\n\nlibrary(ggplot2)\np <- ggplot(df, aes(ymin = 0))\np1 <- p + geom_rect(aes(xmin = wm, xmax = w,\n     ymax = height, fill = x))\n\np2 <- p1 + geom_text(aes(x = wt, y = height *\n     0.5, label = x))\n\np2 + ggtitle(\"Carbon Emission and Gas Price by State\") + xlab(\"Gas Price\") + ylab(\"Carbon Emission\")\n\n\n\n\nThe figure above shows the relationship between carbon emissions and gas prices by state. California has the highest gas price and the least carbon emission. On the other hand, Texas has the least gas price while the highest carbon emission. The figure illustrates that the higher the gas price, the lower the carbon emissions."
  },
  {
    "objectID": "posts/Assignment4/index.html#table-with-embedded-charts",
    "href": "posts/Assignment4/index.html#table-with-embedded-charts",
    "title": "Assignment4",
    "section": "2. Table with Embedded Charts",
    "text": "2. Table with Embedded Charts\n\n# Data Visualization\n# Assignment 4-2\n\n# Creating Table with Embedded Charts\ncarbon <- read.csv(\"carbon.csv\", header=TRUE)\nhead(carbon)\n\n       State PerCapitaEmission CarbonIntensity Region           Division\n1    Alabama         21.648482        433.6878   West            Pacific\n2     Alaska         46.674987        643.1302  South East South Central\n3    Arizona         12.690212        239.3805  South West South Central\n4   Arkansas         21.539954        500.0933   West           Mountain\n5 California          9.081707        140.3001   West            Pacific\n6   Colorado         15.930418        263.3013   West           Mountain\n\nlibrary(Hmisc)\n\nLoading required package: lattice\n\n\nLoading required package: survival\n\n\nLoading required package: Formula\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\nlibrary(tidyverse)\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n✔ purrr   0.3.4      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::lag()       masks stats::lag()\n✖ dplyr::src()       masks Hmisc::src()\n✖ dplyr::summarize() masks Hmisc::summarize()\n\nlibrary(ggplot2)\n\n# Calucating Mean\nmean(carbon$PerCapitaEmission)\n\n[1] 20.29298\n\n\n\n# Creating Dummy Variable\ncarbon$PerCapitaEmission_dummy <- ifelse(carbon$PerCapitaEmission>=20.20489, 1, 0)\n\n# Creating Ordinal Variable\ncarbon$CarbonIntensity_ordinal <- cut2(carbon$CarbonIntensity, m=10)\n\n# \ntable(carbon$PerCapitaEmission_dummy, carbon$CarbonIntensity_ordinal)\n\n   \n    [ 71.2, 197) [196.6, 270) [269.6, 342) [342.4, 500) [500.1,1035]\n  0           11           10           10            3            0\n  1            0            0            0            7           10\n\n\n\ncarbon_df <- data.frame(carbon)\n\n# Code below does not work... \n## p <- ggplot2(carbon_df,aes(CarbonIntensity_ordinal,PerCapitaEmission,fill=as.factor(Division)),size=5)+geom_bar(position=\"dodge\",stat=\"identity\")+facet_wrap(~CarbonIntensity_ordinad,nrow=4)"
  },
  {
    "objectID": "posts/Assignment5/index.html",
    "href": "posts/Assignment5/index.html",
    "title": "Assignment5",
    "section": "",
    "text": "### assignment 5\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n✔ purrr   0.3.4      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(RColorBrewer)\n\n\nhpi <- read.csv(\"hpi_count.csv\")\n\ncolnames(hpi) <- c(\"hpi\", \"gdp\", \"continent\", \"year\")\n\nhpi <- hpi %>% mutate(year = as.character(year))\n\np1 <- ggplot(hpi,\n       aes(x = continent,\n           group = year,\n           fill = year))+\n  scale_fill_brewer(palette = \"Paired\") +\n  geom_bar(position='dodge') +\n  coord_flip() +\n  labs(title = \"Change of High Happiness Frequency by Continent\") +\n  theme_classic()\n\np1\n\n\n\np2 <- ggplot(hpi,\n             aes(x = year,\n                 group = continent,\n                 fill = continent))+\n  scale_fill_manual(breaks = c(\"Asia\", \"Europe\", \"S.Africa\"),\n                    values = c(\"lightgreen\", \"lightblue\", \"orange\")) +\n  geom_bar(position='stack') +\n  labs(title = \"Continents with high happiness index by year\") +\n  theme_classic()\np2\n\n\n\np3 <- ggplot(hpi, aes(x = hpi, fill = continent)) +\n  geom_histogram(binwidth = 15, boundary = -7.5) +\n  coord_polar() +\n  scale_x_continuous(limits = c(20, 70)) +\n  scale_fill_brewer(palette = \"Set2\") +\n  xlab(\"Happy index\")+\n  labs(title = \"Happy index by Continent\")\n\np3\n\nWarning: Removed 6 rows containing missing values (geom_bar)."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]